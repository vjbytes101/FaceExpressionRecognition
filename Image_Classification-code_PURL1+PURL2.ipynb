{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(label_fl):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_fl, 'r') as f:\n",
    "        label1 = f.readlines()\n",
    "        label1 = [l.strip() for l in label1]\n",
    "    label2 = {}\n",
    "    count = 0\n",
    "    for label in label1:\n",
    "        label2[label] = count\n",
    "        count += 1\n",
    "    return label1, label2\n",
    "\n",
    "\n",
    "train_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/train'\n",
    "labels_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/labels.txt'\n",
    "test_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/test'\n",
    "\n",
    "filenames = [file for file in glob.glob(train_path + '*/*')]  #get all filenames\n",
    "\n",
    "labels = [file.split('_')[-1].split('.')[0] for file in filenames ] #get label string name eg. AU1, AU2...\n",
    "\n",
    "label1, label2 = get_label(labels_path) \n",
    "\n",
    "labels = [label2[label] for label in labels if label in label2] #map label  name to id number eg  AU2 -> 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Major defines\n",
    "N_INPUTS  = 32*32*3\n",
    "N_CLASSES = 6\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"c = 0\\nfor i in filenames:\\n    if 'mpg' in i:\\n        c = c+1\\nprint(c)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"c = 0\n",
    "for i in filenames:\n",
    "    if 'mpg' in i:\n",
    "        c = c+1\n",
    "print(c)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input dataset and generate batches of data\n",
    "def dataset_distribution(filenames, labels):\n",
    "    img_str = tf.read_file(filenames)\n",
    "    img_d = tf.image.decode_png(img_str, channels=3)\n",
    "    \n",
    "    #image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_d)\n",
    "    img_d = tf.reshape(float_image , [-1])\n",
    "    return img_d, labels\n",
    "\n",
    "#Train data\n",
    "data = tf.data.Dataset.from_tensor_slices((filenames[0:70000], labels[0:70000]))\n",
    "data = data.map(dataset_distribution, num_parallel_calls = 3)\n",
    "data = data.batch(BATCH_SIZE)\n",
    "data = data.shuffle(buffer_size=2000)    #random number: lower so that buffer filling happens faster\n",
    "iterator = data.make_initializable_iterator()\n",
    "train_batch = iterator.get_next()  #next_element is tensor of (img_train, y_train)\n",
    "\n",
    "# #validation data \n",
    "val = tf.data.Dataset.from_tensor_slices((filenames[70000:], labels[70000:]))   #validation batch\n",
    "val = val.map(dataset_distribution, num_parallel_calls = 3)\n",
    "val = val.batch(2000)\n",
    "validation_iterator = val.make_initializable_iterator()\n",
    "validation_batch = validation_iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "#For test images\n",
    "def _parse_function_test(filenames):\n",
    "    img_str = tf.read_file(filenames)\n",
    "    img_d = tf.image.decode_png(img_str, channels=3)\n",
    "    \n",
    "    #Reserved for image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_d)\n",
    "    img_d = tf.reshape(float_image , [-1])  #flatten\n",
    "    \n",
    "    return img_d\n",
    "\n",
    "filenames_test = [file_t for file_t in glob.glob(test_path + '*/*')]  \n",
    "filenames_test.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "lst = []\n",
    "for i in range(len(filenames_test)):\n",
    "    splt = filenames_test[i].split(\"\\\\\")[2].split('.')[0]\n",
    "    if splt != 'desktop':\n",
    "        lst.append(splt)\n",
    "\n",
    "#For test data\n",
    "data_test = tf.data.Dataset.from_tensor_slices(filenames_test[1:])\n",
    "data_test = data_test.map(_parse_function_test)\n",
    "data_test = data_test.batch(1800)\n",
    "test_iterator = data_test.make_initializable_iterator()\n",
    "test_batch = test_iterator.get_next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model(X, N_CLASSES, reuse, is_training):\n",
    "    \n",
    "    with tf.variable_scope('Conv', reuse = reuse): #to reuse weights and biases for testing\n",
    "        \n",
    "        input_layer = tf.reshape(X, [-1, 32,32,3])\n",
    "\n",
    "        conv0 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=32,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        batchnorm0 = tf.layers.batch_normalization(conv0)\n",
    "        \n",
    "        pool0 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm0,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(\n",
    "          inputs=pool0,\n",
    "          filters=64,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        batchnorm1 = tf.layers.batch_normalization(conv1)\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm1,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "          \n",
    "        dropout0 = tf.layers.dropout(\n",
    "            inputs = pool1,\n",
    "            rate=0.30,\n",
    "            training = is_training)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=dropout0,\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "        batchnorm2 = tf.layers.batch_normalization(conv2)\n",
    "        \n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm2,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding=\"same\",\n",
    "            data_format='channels_last')\n",
    "       \n",
    "        conv3 = tf.layers.conv2d(\n",
    "          inputs=pool2,\n",
    "          filters=256,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        batchnorm3 = tf.layers.batch_normalization(conv3)\n",
    "        \n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm3,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding=\"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        dropout1 = tf.layers.dropout(\n",
    "            inputs = pool3,\n",
    "            rate=0.25,\n",
    "            training = is_training)\n",
    "\n",
    "        flatten = tf.layers.flatten(dropout1)\n",
    "\n",
    "        dense1 = tf.layers.dense(\n",
    "            inputs = flatten,\n",
    "            units = 1024,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "        dense2 = tf.layers.dense(\n",
    "            inputs = dense1,\n",
    "            units = 512,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "        dropout2 = tf.layers.dropout(\n",
    "            inputs = dense2,\n",
    "            rate=0.35,\n",
    "            training = is_training)\n",
    "        \n",
    "        dense3 = tf.layers.dense(\n",
    "            inputs = dropout2,\n",
    "            units = N_CLASSES)\n",
    "        \n",
    "        if is_training: last_layer = dense3     #using sparse cross entropy so no need to apply softmax here\n",
    "        else: last_layer = tf.nn.softmax(dense3)   #for inference\n",
    "\n",
    "        return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = train_batch\n",
    "validation_X, validation_Y = validation_batch\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable = False, name='global_step')\n",
    "\n",
    "out_train = conv_model(X, N_CLASSES, reuse = False, is_training = True)\n",
    "\n",
    "#Cost function as softmax cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits (labels = y, logits = out_train))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost, global_step = global_step)\n",
    "\n",
    "#Train data accuracy\n",
    "out_test = conv_model(X, N_CLASSES, reuse=True, is_training=False)\n",
    "pred = tf.equal(tf.argmax(out_test, 1, output_type=tf.int32), y)\n",
    "acc_train = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "#Validation data accuracy\n",
    "val_output = conv_model(validation_X, N_CLASSES, reuse=True, is_training=False)\n",
    "val_prediction = tf.equal(tf.argmax(val_output, 1, output_type=tf.int32), validation_Y)\n",
    "acc_val = tf.reduce_mean(tf.cast(val_prediction, tf.float32))\n",
    "\n",
    "#test data prediction\n",
    "out_test = conv_model(test_batch, N_CLASSES, reuse=True, is_training=False)\n",
    "test_pred = tf.argmax(out_test, 1 , output_type=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Conv_3/Softmax:0' shape=(?, 6) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    Loss: 0.5080    Train Accuracy: 0.83    Val Accuracy: 0.39    Time: 736.28\n",
      "Epoch 2    Loss: 3.7496    Train Accuracy: 0.53    Val Accuracy: 0.39    Time: 512.91\n",
      "Epoch 3    Loss: 0.7613    Train Accuracy: 0.72    Val Accuracy: 0.39    Time: 511.65\n",
      "Epoch 4    Loss: 0.4631    Train Accuracy: 1.00    Val Accuracy: 0.39    Time: 516.21\n",
      "Epoch 5    Loss: 0.4617    Train Accuracy: 1.00    Val Accuracy: 0.39    Time: 505.31\n",
      "Epoch 6    Loss: 0.8792    Train Accuracy: 0.50    Val Accuracy: 0.60    Time: 504.85\n",
      "Epoch 7    Loss: 0.2774    Train Accuracy: 1.00    Val Accuracy: 0.74    Time: 504.75\n",
      "Epoch 8    Loss: 0.9468    Train Accuracy: 0.66    Val Accuracy: 0.71    Time: 499.13\n",
      "Epoch 9    Loss: 0.1132    Train Accuracy: 1.00    Val Accuracy: 0.71    Time: 500.56\n",
      "Epoch 10    Loss: 1.0068    Train Accuracy: 0.48    Val Accuracy: 0.73    Time: 510.38\n",
      "Epoch 11    Loss: 0.8356    Train Accuracy: 0.91    Val Accuracy: 0.58    Time: 503.14\n",
      "Epoch 12    Loss: 0.4698    Train Accuracy: 0.98    Val Accuracy: 0.69    Time: 505.12\n",
      "Epoch 13    Loss: 0.2651    Train Accuracy: 1.00    Val Accuracy: 0.72    Time: 502.27\n",
      "Epoch 14    Loss: 0.0917    Train Accuracy: 1.00    Val Accuracy: 0.71    Time: 504.86\n",
      "Epoch 15    Loss: 0.0054    Train Accuracy: 1.00    Val Accuracy: 0.69    Time: 501.29\n",
      "Epoch 16    Loss: 0.1003    Train Accuracy: 1.00    Val Accuracy: 0.70    Time: 491.48\n",
      "Epoch 17    Loss: 0.0134    Train Accuracy: 1.00    Val Accuracy: 0.72    Time: 516.21\n",
      "Epoch 18    Loss: 0.0246    Train Accuracy: 1.00    Val Accuracy: 0.74    Time: 566.72\n",
      "Epoch 19    Loss: 0.6518    Train Accuracy: 0.83    Val Accuracy: 0.72    Time: 498.00\n",
      "Epoch 20    Loss: 0.0831    Train Accuracy: 0.98    Val Accuracy: 0.78    Time: 497.19\n",
      "Epoch 21    Loss: 0.0463    Train Accuracy: 1.00    Val Accuracy: 0.81    Time: 496.92\n",
      "Epoch 22    Loss: 0.0224    Train Accuracy: 1.00    Val Accuracy: 0.80    Time: 495.62\n",
      "Epoch 23    Loss: 0.1463    Train Accuracy: 1.00    Val Accuracy: 0.80    Time: 494.93\n",
      "Epoch 24    Loss: 0.0151    Train Accuracy: 1.00    Val Accuracy: 0.81    Time: 506.75\n",
      "Epoch 25    Loss: 0.1201    Train Accuracy: 1.00    Val Accuracy: 0.83    Time: 558.32\n",
      "Epoch 26    Loss: 0.1437    Train Accuracy: 1.00    Val Accuracy: 0.81    Time: 500.90\n",
      "Epoch 27    Loss: 0.0081    Train Accuracy: 1.00    Val Accuracy: 0.79    Time: 482.39\n",
      "Epoch 28    Loss: 0.1929    Train Accuracy: 0.98    Val Accuracy: 0.81    Time: 481.53\n",
      "Epoch 29    Loss: 0.0101    Train Accuracy: 1.00    Val Accuracy: 0.85    Time: 480.54\n",
      "Epoch 30    Loss: 0.0059    Train Accuracy: 1.00    Val Accuracy: 0.82    Time: 482.91\n",
      "Epoch 31    Loss: 0.2672    Train Accuracy: 0.98    Val Accuracy: 0.82    Time: 483.39\n",
      "Epoch 32    Loss: 0.2495    Train Accuracy: 0.86    Val Accuracy: 0.81    Time: 482.56\n",
      "Epoch 33    Loss: 0.0238    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 482.25\n",
      "Epoch 34    Loss: 0.0732    Train Accuracy: 1.00    Val Accuracy: 0.82    Time: 484.32\n",
      "Epoch 35    Loss: 0.0018    Train Accuracy: 1.00    Val Accuracy: 0.83    Time: 482.25\n",
      "Epoch 36    Loss: 0.0858    Train Accuracy: 1.00    Val Accuracy: 0.86    Time: 481.57\n",
      "Epoch 37    Loss: 0.0112    Train Accuracy: 1.00    Val Accuracy: 0.86    Time: 481.01\n",
      "Epoch 38    Loss: 0.2882    Train Accuracy: 0.95    Val Accuracy: 0.86    Time: 482.21\n",
      "Epoch 39    Loss: 0.1402    Train Accuracy: 0.98    Val Accuracy: 0.81    Time: 481.99\n",
      "Epoch 40    Loss: 0.0247    Train Accuracy: 1.00    Val Accuracy: 0.79    Time: 482.77\n",
      "Epoch 41    Loss: 0.0565    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 482.73\n",
      "Epoch 42    Loss: 0.0100    Train Accuracy: 1.00    Val Accuracy: 0.87    Time: 481.67\n",
      "Epoch 43    Loss: 0.0090    Train Accuracy: 1.00    Val Accuracy: 0.86    Time: 482.17\n",
      "Epoch 44    Loss: 0.0013    Train Accuracy: 1.00    Val Accuracy: 0.87    Time: 482.42\n",
      "Epoch 45    Loss: 0.1037    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 482.48\n",
      "Epoch 46    Loss: 0.0016    Train Accuracy: 1.00    Val Accuracy: 0.85    Time: 483.07\n",
      "Epoch 47    Loss: 0.1146    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 484.53\n",
      "Epoch 48    Loss: 0.0446    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 482.64\n",
      "Epoch 49    Loss: 0.0138    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 483.41\n",
      "Epoch 50    Loss: 0.0312    Train Accuracy: 0.98    Val Accuracy: 0.87    Time: 482.96\n",
      "Epoch 51    Loss: 0.0432    Train Accuracy: 1.00    Val Accuracy: 0.88    Time: 481.94\n",
      "Epoch 52    Loss: 0.0858    Train Accuracy: 1.00    Val Accuracy: 0.87    Time: 482.43\n",
      "Epoch 53    Loss: 0.0145    Train Accuracy: 1.00    Val Accuracy: 0.87    Time: 483.37\n",
      "Epoch 54    Loss: 0.0186    Train Accuracy: 1.00    Val Accuracy: 0.87    Time: 483.36\n",
      "Epoch 55    Loss: 0.0013    Train Accuracy: 1.00    Val Accuracy: 0.88    Time: 484.35\n",
      "Epoch 56    Loss: 0.0084    Train Accuracy: 1.00    Val Accuracy: 0.86    Time: 485.71\n",
      "Epoch 57    Loss: 0.0601    Train Accuracy: 1.00    Val Accuracy: 0.88    Time: 486.32\n",
      "Epoch 58    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.88    Time: 483.88\n",
      "Epoch 59    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.87    Time: 487.27\n",
      "Epoch 60    Loss: 0.0720    Train Accuracy: 0.98    Val Accuracy: 0.86    Time: 487.21\n",
      "Epoch 61    Loss: 0.0036    Train Accuracy: 1.00    Val Accuracy: 0.88    Time: 487.55\n",
      "Epoch 62    Loss: 0.0065    Train Accuracy: 1.00    Val Accuracy: 0.88    Time: 487.67\n",
      "\n",
      "Interrupted at epoch 63\n",
      "Done with train\n",
      " saved at C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/DL_model_Save/model-67996\n",
      "Write complete\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 150\n",
    "tr_accuracy = []\n",
    "val_accuracy = []\n",
    "tr_loss = []\n",
    "val_loss = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)   #initialize variables\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    \n",
    "    eCount = 1\n",
    "        \n",
    "    while (eCount < NUM_EPOCHS):\n",
    "        stTime = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(optimizer)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "                training_loss, training_acc = sess.run([cost, acc_train])\n",
    "                tr_accuracy.append(training_acc)\n",
    "                tr_loss.append(training_loss)\n",
    "                \n",
    "                sess.run(validation_iterator.initializer)\n",
    "                validation_acc = sess.run(acc_val)\n",
    "                validation_loss = sess.run(cost)\n",
    "                \n",
    "                tr_loss.append(validation_acc)\n",
    "                val_loss.append(validation_loss)\n",
    "                \n",
    "                print(\"Epoch {}    Loss: {:,.4f}    Train Accuracy: {:,.2f}    Val Accuracy: {:,.2f}    Time: {:,.2f}\"         \n",
    "                      .format(eCount, training_loss, training_acc, validation_acc, time.time() - stTime))\n",
    "                \n",
    "               \n",
    "                eCount += 1\n",
    "                break\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print ('\\nInterrupted at epoch %d' % eCount)\n",
    "            \n",
    "                eCount = NUM_EPOCHS + 1\n",
    "                break\n",
    "    print ('Done with train')\n",
    "    #Save the model\n",
    "    save_path = saver.save(sess, 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/DL_model_Save/model', global_step = global_step )\n",
    "    print (' saved at %s' % save_path) \n",
    "    \n",
    "    sess.run(test_iterator.initializer)\n",
    "    test_predictions = sess.run(test_pred)\n",
    "    pred_csv = open('C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/final_res1'+str(eCount)+'.csv', 'w')\n",
    "    header = ['id','label']\n",
    "    with pred_csv:\n",
    "        writer = csv.writer(pred_csv)\n",
    "        writer.writerow((header[0], header[1]))\n",
    "        for count, row in enumerate(range(test_predictions.shape[0])):\n",
    "            writer.writerow((lst[count], label1[test_predictions[count]]))\n",
    "        print(\"Write complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Final result on test data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/purl2_coding/final_res1151.csv')\n",
    "df['id'] = df['id'].str.split('_')\n",
    "a_count = df[df['id'].str[1]==df['label']].count()[0]\n",
    "total_test_Sample = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.666666666666671"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = (a_count/total_test_Sample)*100\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
